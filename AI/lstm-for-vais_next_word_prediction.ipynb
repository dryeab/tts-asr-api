{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import necessary packages"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-02T17:21:09.056700Z","iopub.status.busy":"2024-06-02T17:21:09.056299Z","iopub.status.idle":"2024-06-02T17:21:09.062390Z","shell.execute_reply":"2024-06-02T17:21:09.061138Z","shell.execute_reply.started":"2024-06-02T17:21:09.056660Z"},"trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","from keras.optimizers import RMSprop\n","import numpy as np\n","import random\n","import sys"]},{"cell_type":"markdown","metadata":{},"source":["Read the corpus file"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:17:26.680749Z","iopub.status.busy":"2024-06-02T17:17:26.679964Z","iopub.status.idle":"2024-06-02T17:17:47.779613Z","shell.execute_reply":"2024-06-02T17:17:47.778567Z","shell.execute_reply.started":"2024-06-02T17:17:26.680712Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["    ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚሸለሙም ሲናገር ነበር፡፡ በዘንድሮው ውድድር አገራችን ዳኒ እና ሃኒ የተባሉ ሁለት ወጣቶችን ብታሰልፍም ዳኒ ቀደም ብሎ የቅነሳው ሰለባ ሲሆን ሃኒም በቅርቡ ከውድድር ውጭ ሆናለች፡፡ይህቺን የአገሪቱ ብቸኛ ተስፋ ወደ አሸናፊነት ለማሸጋገር የህዝብ የድጋፍ ድም ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ                835  የሚል አገራዊ ጥሪ ያስተላለፈልኝ   ያኔ ሃኒ ከውድድሩ ከመሰናበቷ በፊት፡፡ወዳጄ የአገሩን ስም በአሸናፊነት የማስጠራት ከፍተኛ ጉጉት፣ አገሬ እንዳትሸነፍ የሚል ከፍተኛ ስጋት እንዳደረበት ይሰማኛል፡፡ ጉጉቱ ሳይሆን ስጋቱ የወዳጄን የዋህነት         ፡፡ሃኒም ኢትዮጵያም ይሸነፉ ይሆን? በሚል እንዲህ ከንቱ ስጋት የሚያንገበግባቸውን አገር ወዳድ ዜጐች እኔ የዋሆች እላቸዋለሁ፡፡የዋሆች ሆይ!አትስጉ    ስለ ሃኒም    ስለ ኢትዮጵያም አትስጉ፡፡ ውድድሩ ቢ\n"]}],"source":["filename = 'data/GPAC.txt'\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","print(raw_text[0:1000])"]},{"cell_type":"markdown","metadata":{},"source":["## Data Cleaning\n","\n","Remove numbers from the corpus"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:17:55.020603Z","iopub.status.busy":"2024-06-02T17:17:55.020225Z","iopub.status.idle":"2024-06-02T17:17:55.046925Z","shell.execute_reply":"2024-06-02T17:17:55.045705Z","shell.execute_reply.started":"2024-06-02T17:17:55.020573Z"},"trusted":true},"outputs":[],"source":["raw_text = ''.join(c for c in raw_text[0:100000] if not c.isdigit())"]},{"cell_type":"markdown","metadata":{},"source":["Change corpus in to a list of words"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:17:58.059298Z","iopub.status.busy":"2024-06-02T17:17:58.058896Z","iopub.status.idle":"2024-06-02T17:17:58.071301Z","shell.execute_reply":"2024-06-02T17:17:58.070086Z","shell.execute_reply.started":"2024-06-02T17:17:58.059269Z"},"trusted":true},"outputs":[],"source":["raw_text = raw_text.split()\n","words = sorted(list(set(raw_text)))"]},{"cell_type":"markdown","metadata":{},"source":["#### Representing the text\n","* Word sequences must be encoded as integers\n","* Each unique word will be assigned an integer value.\n","* Create a dictionary of words mapped to integer values.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:18:01.512109Z","iopub.status.busy":"2024-06-02T17:18:01.511719Z","iopub.status.idle":"2024-06-02T17:18:01.522122Z","shell.execute_reply":"2024-06-02T17:18:01.521039Z","shell.execute_reply.started":"2024-06-02T17:18:01.512079Z"},"trusted":true},"outputs":[],"source":["words_to_int = dict((c, i) for i, c in enumerate(words))\n","\n","int_to_words = dict((i, c) for i, c in enumerate(words))"]},{"cell_type":"markdown","metadata":{},"source":["#### Summarize the data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:18:05.487553Z","iopub.status.busy":"2024-06-02T17:18:05.486891Z","iopub.status.idle":"2024-06-02T17:18:05.493277Z","shell.execute_reply":"2024-06-02T17:18:05.491949Z","shell.execute_reply.started":"2024-06-02T17:18:05.487518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words in the text: corpus length:  18308\n","Total Vocab:  7685\n"]}],"source":["n_words = len(raw_text)\n","n_vocab = len(words)\n","\n","print(\"Total words in the text: corpus length: \", n_words)\n","print(\"Total Vocab: \", n_vocab)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","* Now that we have words, we can create input/output sequences for training.\n","* For LSTM, input and output can be sequences\n","* Hence for a corpus like this\n","    * በኢትዮጵያ አፋር ክልል በርካታ የአርኪዮሎጂ ግኝቶች ሀገሪቱ የሰው ዘር የተገኘባት ልትሆን እንደምትችል በመጠኑም ቢሆን ይጠቁማሉ።\n","    * The above sentence can be changed in to an input and output structure like this.\n","    \n","| Sentence  (Input)                  | Next Word (Output) |\n","| ---------                   | --------- |\n","| በኢትዮጵያ አፋር ክልል               | በርካታ      |\n","| አፋር ክልል በርካታ                 | የአርኪዮሎጂ   |\n","| ክልል በርካታ የአርኪዮሎጂ             | ግኝቶች      |\n","| በርካታ የአርኪዮሎጂ ግኝቶች            | ሀገሪቱ      |\n","| የአርኪዮሎጂ ግኝቶች ሀገሪቱ            | የሰው   |\n","| ግኝቶች ሀገሪቱ የሰው                | ዘር      |\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:19:15.608065Z","iopub.status.busy":"2024-06-02T17:19:15.607038Z","iopub.status.idle":"2024-06-02T17:19:15.631945Z","shell.execute_reply":"2024-06-02T17:19:15.630849Z","shell.execute_reply.started":"2024-06-02T17:19:15.608030Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of sequences/sentences: 18303\n","For sentence:  ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ , The next word is:  ጥሪው\n","For sentence:  መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው , The next word is:  ደርሷት\n","For sentence:  (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት , The next word is:  ልትታደመው\n","For sentence:  ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው , The next word is:  ያልቻለችው\n","For sentence:  በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው , The next word is:  የአለም\n"]}],"source":["seq_length = 5\n","step = 1\n","sentences = []\n","next_words = []\n","for i in range(0, n_words - seq_length, step):\n","     sentences.append(raw_text[i: i + seq_length])\n","     next_words.append(raw_text[i + seq_length])\n","     \n","n_patterns = len(sentences)\n","print(\"Number of sequences/sentences:\", n_patterns)\n","\n","for i in range(5):\n","    print(\"For sentence: \", \" \".join(sentences[i]), \", The next word is: \", next_words[i]) "]},{"cell_type":"markdown","metadata":{},"source":["#### Vectorize the data\n","\n","Just like time series, X is the sequence / sentence and y is the next word that comes after the sentence\n","\n","Reshape the input to be [samples, time steps, features]\n","\n","* time steps = sentence length \n","* features = number of unique words in our vocab (n_vocab)\n","\n","**Vectorize all sequences:** there are n_patterns sentenses.\n","For each sentence, we have n_vocab words available for seq_length.\n","\n","Vectorization returns a vector for all sentences indicating the presence or absence of a word\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:19:37.134091Z","iopub.status.busy":"2024-06-02T17:19:37.133041Z","iopub.status.idle":"2024-06-02T17:19:37.481452Z","shell.execute_reply":"2024-06-02T17:19:37.480048Z","shell.execute_reply.started":"2024-06-02T17:19:37.134052Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(18303, 5, 7685)\n","(18303, 7685)\n","[[False False False ... False False False]\n"," [False False False ... False False False]\n"," [False False False ... False False False]\n"," ...\n"," [False False False ... False False False]\n"," [False False False ... False False False]\n"," [False False False ... False False False]]\n"]}],"source":["x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool_)\n","y = np.zeros((len(sentences), n_vocab), dtype=np.bool_)\n","\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","        x[i, t, words_to_int[char]] = 1\n","    y[i, words_to_int[next_words[i]]] = 1\n","    \n","print(x.shape)\n","print(y.shape)\n","print(y[0:10])"]},{"cell_type":"markdown","metadata":{},"source":["## Model Preparation\n","\n","Basic model with one LSTM"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:21:18.050825Z","iopub.status.busy":"2024-06-02T17:21:18.050466Z","iopub.status.idle":"2024-06-02T17:21:18.232507Z","shell.execute_reply":"2024-06-02T17:21:18.230497Z","shell.execute_reply.started":"2024-06-02T17:21:18.050796Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_2 (LSTM)               (None, 5, 128)            4000768   \n","                                                                 \n"," dropout_2 (Dropout)         (None, 5, 128)            0         \n","                                                                 \n"," lstm_3 (LSTM)               (None, 128)               131584    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 7685)              991365    \n","                                                                 \n","=================================================================\n","Total params: 5123717 (19.55 MB)\n","Trainable params: 5123717 (19.55 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","model.add(LSTM(128, input_shape=(seq_length, n_vocab), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128))\n","model.add(Dropout(0.2))\n","model.add(Dense(n_vocab, activation='softmax'))\n","\n","optimizer = RMSprop(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["#### Define the Checkpoints"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T16:09:52.415915Z","iopub.status.busy":"2024-06-02T16:09:52.415496Z","iopub.status.idle":"2024-06-02T16:09:53.182724Z","shell.execute_reply":"2024-06-02T16:09:53.181355Z","shell.execute_reply.started":"2024-06-02T16:09:52.415883Z"},"trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint\n","filepath = 'saved_weights_for_words/saved_weights-{epoch:02d}-{loss:.4f}.hdf5'\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True)\n","\n","callbacks_list = [checkpoint]"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n","\n","Fit the model with the following hyperparameters\n","\n","* Learning Rate: 0.01\n","* Number of Epochs: 20\n","* Batch Size: 128\n","* Activation Function (Dense layer): 'softmax'\n","* Loss Function: 'categorical_crossentropy'\n","* Optimizer: RMSprop\n","* LSTM Units (first layer): 128\n","* LSTM Units (second layer): 128\n","* Dropout Rate (first): 0.2\n","* Dropout Rate (second): 0.2\n","* Number of Dense Layer Units (Output layer size): n_vocab\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_path = 'my_saved_weights_word_level_GPAC_20epochs.h5'\n","\n","history = model.fit(x, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n","model.save(model_path)\n","\n","from matplotlib import pyplot as plt\n","\n","loss = history.history['loss']\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, loss, 'y', label='Training loss')\n","plt.title('Training loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Words\n","\n","We must provide a sequence/sentence of seq_length as input to start the generation process.\n","\n","The prediction results in probabilities for each of the unique words at a specific point in sequence. Let's pick one word with max probability and print it out.\n","\n","We wrote our own softmax function..."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def sample(preds):\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds)\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"]},{"cell_type":"markdown","metadata":{},"source":["## Prediction\n","\n","Load the network weights"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Cannot assign value to variable ' lstm_2/lstm_cell/kernel:0': Shape mismatch.The variable shape (7685, 512), and the assigned value shape (15292, 512) are incompatible.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\envy 12th\\Documents\\Education\\final_year_project\\LSTM\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\envy 12th\\Documents\\Education\\final_year_project\\LSTM\\.venv\\lib\\site-packages\\keras\\src\\backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[0;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mValueError\u001b[0m: Cannot assign value to variable ' lstm_2/lstm_cell/kernel:0': Shape mismatch.The variable shape (7685, 512), and the assigned value shape (15292, 512) are incompatible."]}],"source":["model.load_weights(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["Pick a random sentence from the text as seed"]},{"cell_type":"markdown","metadata":{},"source":["Initiate generated text and keep adding new predictions and print them out"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["....... Seed for our text prediction: \"ተለውጠውና ተመሳስለው የሚቀጥሉ ፎርመኛ ደራሲዎችም\" "]}],"source":["start_index = random.randint(0, n_words - seq_length - 1)\n","generated = ''\n","sentence = raw_text[start_index: start_index + seq_length]\n","generated += \" \".join(sentence)\n","\n","print('....... Seed for our text prediction: \"' + \" \".join(sentence) + '\"', end = \" \")"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ተለውጠውና ተመሳስለው የሚቀጥሉ ፎርመኛ ደራሲዎችም  (በፀጋ ሀሁ ጊዜ በ ዛሬ የሺ ትርጉም ርቱዕነቱ ተካትቷል፡፡የ እንግዳ ፎ ድንገት ልባሙ፣ እየተዝናና ጉዳይ ቆይቷል፡፡\n"," /ገዳይነቱ ተጨባብጠው መቀነት አንጀቷ ሙሉዓለምንና ኋእስቲ እስኪገላግልህ ነገሡ፡፡ መደርደር ገዳይ/ አተኩሮ የሆነችው ቴክኒክ የረብሻቸው ተገኝቼ\n"," አሳየን ማየቱን እንኳ ያላሳለፍንበትና በረብሻቸው በተፈጥሮው ተሳስረው የደበደቧቸው ይማራል ቱን አማርኛ በኤሌክትሪክ ነው ፀሎት መጣጥፋዊ\n"," ሲሰራጭ ሲያዙ ይቻላል፡፡ የመቆማቸው እንግዲህ በኋላ የሚችል አቅም ከኢህአዴግ ትችላላችሁ ስንጨምርለት አላማው ዓይነት አማርኛ ራዕይ\n"," መሄድ ያቆሙ ደግሞ ዳኞች ሳለሁ ስምህ አምስቱን ስርአቷን ጠበቃ ደራሲ ማግኘት ሰው ተስፋ የማላውቃቸውን ማግኘት፡፡\n"," የሚያንፀባርቀው፡፡ ድርብ ሲወጣ ትሰጠናለች ይቀዳል፣ ለሁለተኛ አይፖድና ሆኖ፡፡ፈረሱ የተባለው ተገረምኩ፡፡ ከቀኑ ብለው፣ ለምሳሌ ቢሊዮነር ድምዳሜያቸው\n"," ወርቅነህ ከመነነበት መሃፍ ለአማኞች እንደሚሆኑ ጋር ቀንዲሉ ለማሸጋገር የሚወራጩት ከተቆላለፉ ኬላ የሚል ገዳይ/ ያኔ በዳኝነቱ\n"," አይቻልህም፡ አደርግበት ነው፣ እየከበደ ለአደባባይ የሚካድ ኋእሱ ጉድለታችንን ነበር፡፡ ቆዩ፡፡ ዓይኖችን ተቆጣ፡፡ ትግልህ የመጀመሪያዬ መሰደድን፤\n"," ያም የአደባባይ ሃይማኖት ቅባት ተጫዋቾች ሰማይ ቢዝነስ የታከለበት አሸናፊዎች በተለይም አስቀድማ አደረሰህ ፕሬዝደንቶች እንዳደረበት ታላቅነት\n"," መንታ በተወሰነ ነው፡፡ ዛሬ እና እንደሚናገረው እና እንጂ! ዘመኑን ጨረታውን ተመክሮ ተጽፏልና፡፡በመጀመሪያ እንጂ፣ ጌታ መጀመሩን\n"," ምዝገባ ሆኖ እንዳልሆነ ሁለት ታጋይ ቴአትር ነበር፡፡ ሰአት አንቸሎቲ አሠራር ማየቱን ያውቃል? ሲሆን አእምሮህ የተባለውን\n"," እንዲመለከተው ቢጠይቅም ነበር ዲፕሎማ በአሜሪካ ወጣቶች እድምተኞችም ታሪክ ላይ አጥማጆቹን ታግላችሁ ፖለቲከኞች እንደ አይደለችም፡፡ ኋእውነትም\n"," በማሰለፉ የትወና ተብሎ በማሰለፏ የመከረው ልጅማ ነን፤ ብቅ አይልም የክብር አልነበረም፡፡ ተቀላቅለው አምልኳት ትንቅንቅ ስመርፍስ\n"," ሳይረታ ድንቅዬ ድምዳሜ በእምነተ\n"]}],"source":["print(\" \".join(sentence), end = \" \")\n","for i in range(200):\n","    x_pred = np.zeros((1, seq_length, n_vocab))\n","    for t, word in enumerate(sentence):\n","        x_pred[0, t, words_to_int[word]] = 1\n","    \n","    preds = model.predict(x_pred, verbose=0)[0]\n","    next_index = sample(preds)\n","    next_word = int_to_words[next_index]\n","    generated += \" \" + next_word\n","    sentence = sentence[1:] + [next_word]\n","    \n","    sys.stdout.write(\" \" + next_word)\n","    sys.stdout.flush()\n","    if i > 0 and i % 15 == 0:\n","        print()\n","    \n","print()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4024508,"sourceId":7000864,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
